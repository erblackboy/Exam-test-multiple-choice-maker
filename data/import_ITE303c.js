window.ite303c_questions = [
    {
        id: "ite303c_001",
        question: "A process or set of rules to be followed in calculations or other problem-solving operations, especially by a computer describes an",
        options: [
            "A. set of inputs",
            "B. troubleshooting problem",
            "C. algorithm",
            "D. computer program"
        ],
        answer: [2],
        explanation: "An algorithm is defined as a process or set of rules to be followed in calculations or other problem-solving operations, especially by a computer"
    },
    {
        id: "ite303c_002",
        question: "To make this algorithm functional, which step would you add to step 4?\n1. Scan to find the smallest number\n2. Set to 0 in the index in the output array\n3. Remove that number from the input array\n4. _________________________________________",
        options: [
            "A. Divide the array by the index, print the array output",
            "B. Repeat steps 1-3, but subtract the total from the numbers summated",
            "C. Print output in the correct order",
            "D. Repeat steps 1-3, but add 1 to the index number for each loop"
        ],
        answer: [3],
        explanation: "Repeating the steps, but adding 1 to the index number will ensure that the new array will sort the numbers in the correct order"
    },
    {
        id: "ite303c_003",
        question: "Take input, get output, use output on next input is an example of a learning algorithm",
        options: [
            "A. true",
            "B. false"
        ],
        answer: [0],
        explanation: "This is true, because a learning algorithm is able to use the output's \"learnings\" on the next input to improve function performance"
    },
    {
        id: "ite303c_004",
        question: "Learning algorithms require large datasets, which means storing identifying information about users",
        options: [
            "A. true",
            "B. false"
        ],
        answer: [0],
        explanation: "This is true. One side effect of learning algorithms is that they require large amounts of data to get better at what they do. This is usually a negative for users, which end up being victims of having their data tracked"
    },
    {
        id: "ite303c_005",
        question: "A _________ _________ in machine learning is the idea that the algorithm itself influences the next set of inputs that go into the model. The main takeaway is that algorithms sometimes have more influence than a user's initial input.",
        options: [
            "A. false dichotomy",
            "B. feedback loop",
            "C. learning algorithm",
            "D. large dataset"
        ],
        answer: [1],
        explanation: "A feedback loop in machine learning is the idea that the algorithm itself influences the next set of inputs that go into the model. The main takeaway is that algorithms sometimes have more influence than a user's initial input."
    },
    {
        id: "ite303c_006",
        question: "Are anonymous datasets truly anonymous?",
        options: [
            "A. No, due to lacking regulations",
            "B. Yes, thanks to machine learning randomness algorithms",
            "C. Yes, thanks to privacy regulations",
            "D. No, due to combining data and re-identification"
        ],
        answer: [3],
        explanation: "Anonymized datasets assume that the dataset exists in a vacuum, and researchers don't anticipate what the individual row characteristics will look like when combined with other datasets"
    },
    {
        id: "ite303c_007",
        question: "Which of the following best describes what an algorithm is?",
        options: [
            "A. a list of ingredients a computer uses to generate problems to solve",
            "B. a type of computer that calculates problem-solving methods",
            "C. a type of process a human uses to write down what steps need to happen to get a problem solved",
            "D. a recipe that a computer uses to solve problems"
        ],
        answer: [3],
        explanation: "An algorithm is a process or set of rules to be followed in calculations or other problem-solving operations, especially by a computer."
    },
    {
        id: "ite303c_008",
        question: "An algorithm that takes an input, tries 10 different sorting techniques, and uses the best fit on the next 100 inputs is best described as a",
        options: [
            "A. implicit algorithm",
            "B. learning algorithm",
            "C. data algorithm",
            "D. explicit algorithm"
        ],
        answer: [1],
        explanation: "A learning algorithm is one that takes the initial calculation to get better at solving the problem at hand, applying the data from the learned output to the next set of inputs"
    },
    {
        id: "ite303c_009",
        question: "Which of these steps follows the most logical order for a low-to-high sorting algorithm?",
        options: [
            "A. 1. Scan to find the largest number\n2. Set to 0 in the index in the output array\n3. Remove that number from the input array\n4. Repeat steps 1-3, but add 1 to the index number for each loop",
            "B. 1. Scan to find the smallest number\n2. Set the length of the array in the index in the output array\n3. Remove that number from the input\n4. Repeat steps 1-3, but add 1 to the index number for each loop",
            "C. 1. Scan to find the smallest number\n2. Set to 0 in the index in the output array\n3. Remove that number from the input array\n4. Repeat steps 1-3, but add 1 to the index number for each loop",
            "D. 1. Scan to find the smallest number\n2. Set to 0 in the index in the output array\n3. Remove that number from the input array"
        ],
        answer: [2],
        explanation: "The goal of a sorting algorithm is to find the smallest number each time, identifying it's place in the output array (0, 1, 2, etc) and removing it from the input array"
    },
    {
        id: "ite303c_010",
        question: "What's the difference between a basic and learning algorithm?",
        options: [
            "A. A basic algorithm takes an input, while a learning algorithm takes an input and gets an output",
            "B. A basic algorithm takes an input, gets an output, while a learning algorithm takes multiple inputs and gets multiple outputs",
            "C. A basic algorithm takes an input and gets an output, while a learning algorithm takes multiple inputs and gets multiple outputs",
            "D. An basic algorithm takes an input and gets an output, while a learning algorithm uses the output on the next input"
        ],
        answer: [3],
        explanation: "A basic algorithm takes an input, makes a calculation, and provides an output. A learning algorithm uses the calculation to get better at solving the problem at hand, applying the data from the learned output to the next set of inputs"
    },
    {
        id: "ite303c_011",
        question: "Pseudocode can best be defined as",
        options: [
            "A. a middle ground between code and plain writing that can be fed into a computer",
            "B. a type of Javascript that is both human and machine-readable",
            "C. a Python library for machine learning",
            "D. an explainable description of code that is meant for humans, not computers"
        ],
        answer: [3],
        explanation: "Pseudocode is a helpful notation that explains how code should run in human-readable format. It's often used in machine learning development to guide transparanecy in how the model develops"
    },
    {
        id: "ite303c_012",
        question: "What side effect of learning algorithms creates an ethical dilemma for its users?",
        options: [
            "A. Learning algorithms require large amounts of computing power, which is bad for the environment",
            "B. Learning algorithms are costly to run, which drives up prices for consumer services",
            "C. Learning algorithms require government regulation, which is bad for software developers",
            "D. Learning algorithms require large datasets, which means storing identifying information about users"
        ],
        answer: [3],
        explanation: "By it's nature, a learning algorithm requires data to be collected to make the underlying service more useful. To reach it's goal, which in the real world is usually optimizing for clicks/engagement, a side effect of these algorithms is that they turn the users into individual, large datasets"
    },
    {
        id: "ite303c_013",
        question: "How do anonymized datasets fall short of their goal of being anonymous?",
        options: [
            "A. Anonymized datasets can be traced back to the individuals by looking at their browsing history in the app",
            "B. Anonymized datasets aren't actually anonymous because many of the data fields can identify a user",
            "C. Anonymized datasets can be re-identifyed by anyone holding the hash key",
            "D. Anonymized datasets can be combined with other datasets, which can re-identify individuals"
        ],
        answer: [3],
        explanation: "Anonymized datasets assume that the dataset exists in a vacuum, and researchers don't anticipate what the individual row characteristics will look like when combined with other datasets"
    },
    {
        id: "ite303c_014",
        question: "What is a likely outcome for a weather app using a learning algorithm to figure out where to put their future weather stations?",
        options: [
            "A. Collecting weather data every time the app is opened, knowing the temperature where the app is being used",
            "B. Accessing weather forecasts from local broadcasts",
            "C. Storing data in an AWS instance with all weather stations in the country",
            "D. Collecting location data every time the app is opened, potentially learning where a user lives, works, etc."
        ],
        answer: [3],
        explanation: "To be useful, machine learning algorithms need to collect lots of data. For this weather app model to predict where to put future weather stations, they would need to collect at least every location request when the app is used."
    },
    {
        id: "ite303c_015",
        question: "Which of the following is a good example of a feedback loop in machine learning?",
        options: [
            "A. A shopping app surfaces new items to buy, which is based on dataset from customers fitting a similar profile. When you buy, you go into that dataset",
            "B. A social media site surfaces controversial posts, which make users more angry and lead to more angry posts on the network",
            "C. A shopping app tracks your purchases, and recommends new things to buy",
            "D. A social media site tracks engagement, uses an algorithm to surface posts you're likely to engage with, which then goes back into the algorithm"
        ],
        answer: [3],
        explanation: "A feedback loop in machine learning is the idea that the algorithm itself influences the next set of inputs that go into the model. The main takeaway is that algorithms sometimes have more influence than a user's initial input."
    },
    {
        id: "ite303c_016",
        question: "A fact of learning algorithms is that",
        options: [
            "A. even if you haven't shared an direct datapoint about yourself, with enough related datapoints the algorithm can make an educated guess with alarming accuracy",
            "B. just because they are capable of improving outputs, they don't need more inputs",
            "C. they only learn when given small amounts of data, and without the proper training sample, the results can be wildly inaccurate",
            "D. they cannot make predictions with current technology"
        ],
        answer: [0],
        explanation: "Machine learning, when given enough datapoints, can begin to predict other datapoints with incredible accuracy. For example, an algorithm could make a very accurate prediction of where a person lives, given coordinates of where they shop, eat out, and go to the movies"
    },
    {
        id: "ite303c_017",
        question: "A basic learning model can figure out which of the 10 sorting mechanisms works best for this type of input. A complex model ______________________",
        options: [
            "A. automatically scans all inputs",
            "B. automatically derives its mechanism",
            "C. can figure out up to 150 mechanisms",
            "D. can figure out up to 50 mechanisms"
        ],
        answer: [1],
        explanation: "A basic learning model can figure out which of the 10 sorting mechanisms works best for this type of input. A complex model automatically derives its mechanism"
    },
    {
        id: "ite303c_018",
        question: "A model's error rate is the ratio of incorrect predictions to total predictions",
        options: [
            "A. true",
            "B. false"
        ],
        answer: [0],
        explanation: "This is true. A model's error rate is the ratio of incorrect predictions to total predictions, usually displayed as a percentage. This is used to judge how accurate a model will be going forward"
    },
    {
        id: "ite303c_019",
        question: "The goal of the _______________ is to get the model's error rate as low as possible. To do this, we repeat a cycle of feeding training data, compare predictions to actual outcomes, and adjust the model as needed.",
        options: [
            "A. develop phase",
            "B. deployment phase",
            "C. training phase",
            "D. algorithmic phase"
        ],
        answer: [2],
        explanation: "The goal of the training phase is to adjust the model based on a subset of data, optimizing for a lower error rate"
    },
    {
        id: "ite303c_020",
        question: "As models become more complex, researchers are unable to reason why the decisions are being made. This is called the _________________",
        options: [
            "A. black box problem",
            "B. rationality phase",
            "C. training phase",
            "D. false input problem"
        ],
        answer: [0],
        explanation: "As models become more complex, researchers are unable to reason why the decisions are being made. This is called the black box problem because the model is opaque, and we can only see inputs and outputs"
    },
    {
        id: "ite303c_021",
        question: "____________ can often be caused by predicting what someone may or may not do based on data",
        options: [
            "A. real harm",
            "B. dataset collection",
            "C. irrationality",
            "D. retraining scenarios"
        ],
        answer: [0],
        explanation: "Real harm can be caused by predictive models. For example, a model used by a bank wrongly predicts a person will not be able to pay off a credit card bill. Denying that person credit even though they are creditworthy is a harmful action"
    },
    {
        id: "ite303c_022",
        question: "Hedge funds largely rely on predictive models to judge the movement of stocks, bonds, and securities",
        options: [
            "A. true",
            "B. false"
        ],
        answer: [0],
        explanation: "This is true. Hedge funds use predictive models to judge the movement of stocks, bonds, and securities. They make money by placing bets based on these predictions"
    },
    {
        id: "ite303c_023",
        question: "What's the difference between a basic and complex learning algorithm?",
        options: [
            "A. A basic algorithm cannot use computer vision, while a complex algorithm can",
            "B. A basic algorithm has a set amount of choices to optimize for, while a complex algorithm is given the freedom to find its own model",
            "C. A basic algorithm cannot process more than 5 steps in a function, while a complex algorithm can process up to 15",
            "D. A basic algorithm can handle simple inputs like numbers, while a complex algorithm can handle complex inputs like pictures"
        ],
        answer: [1],
        explanation: "A basic learning model can figure out which of the 10 sorting mechanisms works best for this type of input. A complex model automatically derives its mechanism"
    },
    {
        id: "ite303c_024",
        question: "When building a predictive model, what is the goal of the develop phase?",
        options: [
            "A. To specify the type of algorithm the model should use and make sure the data is cleaned/formatted",
            "B. To get the model to accept new inputs, train, and repeat training until it finds a better curve",
            "C. To get the model's error function below an acceptable percentage",
            "D. To plug in 40% of your dataset, testing the model's accuracy"
        ],
        answer: [0],
        explanation: "Before training a model, it must be developed with a type of algorithm defined, and with clean, formatted data. This way the algorithm knows how to read the data attributes, as well as how to model it"
    },
    {
        id: "ite303c_025",
        question: "When building a predictive model, what is the goal of the training phase?",
        options: [
            "A. To adjust training methods from backpropagation to supervised learning to see how that affects outputs",
            "B. To use the model in real-world scenarios, monitoring performance",
            "C. To adjust the model based on a subset of data, optimizing for a lower error rate",
            "D. To specify the type of algorithm the model should use and make sure the data is cleaned/formatted"
        ],
        answer: [2],
        explanation: "The goal of the training phase is to get the model's error rate as low as possible. To do this, we repeat a cycle of feeding training data, compare predictions to actual outcomes, and adjust the model as needed."
    },
    {
        id: "ite303c_026",
        question: "When building a predictive model, what is the goal of the deployment phase?",
        options: [
            "A. To plug in 40% of your dataset, testing the model's accuracy",
            "B. To get the model to accept new inputs, train, and repeat training until it finds a better curve",
            "C. To specify the type of algorithm the model should use and make sure the data is cleaned/formatted",
            "D. To use the model in real-life predictions, monitoring the error rate and accuracy"
        ],
        answer: [3],
        explanation: "Once a model has been trained properly, it's ready for use. Once deployed, a researcher monitors the model's performance to make sure the error rate stays below the accepted threshold"
    },
    {
        id: "ite303c_027",
        question: "What are the attributes of an error function when training a predictive model",
        options: [
            "A. the ratio of algorithm to curve in a predictive model",
            "B. the percentage of data that is formatted properly",
            "C. the percentage of predictions that don't match actual outcomes",
            "D. the ratio of training data to actual data the model has consumed"
        ],
        answer: [2],
        explanation: "A model's error rate is the ratio of incorrect predictions to total predictions, usually displayed as a percentage. This is used to judge how accurate a model will be going forward"
    },
    {
        id: "ite303c_028",
        question: "In a complex learning function, we will understand the ____, but not the ____",
        options: [
            "A. inputs/outputs, algorithm",
            "B. algorithm, cause and effect",
            "C. input data, output data",
            "D. causal link, correlation"
        ],
        answer: [0],
        explanation: "In creating a model, we can change the inputs and assess the outputs, but cannot program the algorithm itself, which is automatically derived from the model"
    },
    {
        id: "ite303c_029",
        question: "What is the black box problem?",
        options: [
            "A. The problem created when researchers don't create accurate attributes for a model",
            "B. When a model is deployed, but researchers are unable to figure out why it's making decisions",
            "C. The issue of not having enough data to accurately train a model",
            "D. When a model cannot accurately judge shape or color of objects due to missing data"
        ],
        answer: [1],
        explanation: "As models become more complex, researchers are unable to reason why the decisions are being made. This is called the black box problem because the model is opaque, and we can only see inputs and outputs"
    },
    {
        id: "ite303c_030",
        question: "Which of the following is a negative consequence of a predictive model used in real life?",
        options: [
            "A. A model used by a lab indicates a person is in danger",
            "B. A model used by a lab wrongly predicts a person will not be able to pay their credit card",
            "C. A model used by a bank accurately predicts a person will not be able to pay off a loan",
            "D. A model used by a bank wrongly predicts a person will not be able to pay off a loan"
        ],
        answer: [3],
        explanation: "A example of a negative consequence of predictive models in real life is wrongly denying someone a loan or line of credit who would have been able to pay it off"
    },
    {
        id: "ite303c_031",
        question: "How are predictive models used in hedge funds?",
        options: [
            "A. they provide predictions to shareholders to estimate returns",
            "B. they predict whether people will be able to pay off loans, and then provide loans",
            "C. they aid researchers by forecasting financial collapse",
            "D. they predict future movement of stocks and find points to exploit the market moving in either direction"
        ],
        answer: [3],
        explanation: "Hedge funds use predictive models to judge the movement of stocks, bonds, and securities. They make money by placing bets based on these predictions"
    },
    {
        id: "ite303c_032",
        question: "What is one possible reason a model may predict a higher crime rate based on datasets used?",
        options: [
            "A. The model's training curve was not provided enough data",
            "B. If drug arrests are historically high in that area, the model may correlate crime with areas of high drug use based on the datasets",
            "C. If crime is down in an area, a model may predict a parabolic curve which estimates crime is due to rise again",
            "D. If a dataset isn't properly formatted, crime may be linked to the error function, outputting false data"
        ],
        answer: [1],
        explanation: "Models can only make predictions based on the datasets provided. In this example, the correlation of drug and crime data historically would be enough for the model to predict higher crime in areas of high drug arrests"
    },
    {
        id: "ite303c_033",
        question: "To measure accuracy, take the number of ________ results and divide over the number of all results",
        options: [
            "A. true positive and true negative",
            "B. true positive and false negative",
            "C. false positive and false negative",
            "D. false positive and true negative"
        ],
        answer: [0],
        explanation: "To measure accuracy, take the number of correct predictions (true positive and true negative) and divided over the total number of predictions (true positive, true negative, false positive, false negative)"
    },
    {
        id: "ite303c_034",
        question: "A false negative result is one in which the model predicts a result was negative, and in reality it was ___________. It is an _________ prediction",
        options: [
            "A. negative, correct",
            "B. positive, correct",
            "C. positive, incorrect",
            "D. negative, incorrect"
        ],
        answer: [2],
        explanation: "A false negative result is one in which the model predicts a result was negative, and in reality it was positive. It is an incorrect prediction"
    },
    {
        id: "ite303c_035",
        question: "City and State are correlated data, but a model will measure no variation and the results will not be affected",
        options: [
            "A. true",
            "B. false"
        ],
        answer: [1],
        explanation: "This is false. City and State are correlated data, as the provided city always resides in the same state. If we keep these two inputs separate in a model, it will lead to correlation errors."
    },
    {
        id: "ite303c_036",
        question: "A ________ training set relies on running a final accuracy test before deploying a model. An __________ training set relies on multiple tests to ensure that a model is free of bias",
        options: [
            "A. classic, optimized",
            "B. test, classic",
            "C. sample, optimized",
            "D. optimized, classic"
        ],
        answer: [0],
        explanation: "A classic training set relies on running a final accuracy test before deploying a model. An optimized training set relies on multiple tests to ensure that a model is free of bias"
    },
    {
        id: "ite303c_037",
        question: "An unknown unknown is an example of a cultural reflection of data",
        options: [
            "A. true",
            "B. false"
        ],
        answer: [1],
        explanation: "This is false. An unknown unknown is an example of a empirical reflection of data"
    },
    {
        id: "ite303c_038",
        question: "An ethical predictive model needs to be accurate, _____________, and fair",
        options: [
            "A. predictable",
            "B. moral",
            "C. truthful",
            "D. explainable"
        ],
        answer: [3],
        explanation: "An ethical predictive model needs to go beyond the typical guideline for accuracy. We need it to be fair (adjusting for potential blind spots/bias) and explainable (avoiding the black box problem)"
    },
    {
        id: "ite303c_039",
        question: "To measure a predictive model's accuracy, you",
        options: [
            "A. multiply the number of total predictions by the percentage of correct predictions",
            "B. measure the ratio of the model's error curve",
            "C. divide the number of predictions by the total dataset",
            "D. divide the number of correct predictions by the total number of predictions"
        ],
        answer: [3],
        explanation: "To measure accuracy, take the number of correct predictions (true positive and true negative) and divided over the total number of predictions (true positive, true negative, false positive, false negative)"
    },
    {
        id: "ite303c_040",
        question: "A predictive model's false negative result can be defined as",
        options: [
            "A. the predicted result was negative, and the actual result was negative",
            "B. the predicted result was positive, and the actual result was negative",
            "C. the predicted result was negative, and the actual result was positive",
            "D. the predicted result was positive, and the actual result was positive"
        ],
        answer: [2],
        explanation: "A false negative result is one in which the model predicts a result was negative, and in reality it was positive. It is an incorrect prediction"
    },
    {
        id: "ite303c_041",
        question: "A predictive model's true positive result can be defined as",
        options: [
            "A. the predicted result was positive, and the actual result was negative",
            "B. the predicted result was negative, and the actual result was positive",
            "C. the predicted result was positive, and the actual result was positive",
            "D. the predicted result was negative, and the actual result was negative"
        ],
        answer: [2],
        explanation: "A true positive result is one in which the model predicts a positive result, and in reality, the result was positive. It is a correct prediction"
    },
    {
        id: "ite303c_042",
        question: "Model inputs of address with \"City + State\" as separate inputs from a dataset would violate which accuracy guideline?",
        options: [
            "A. Objective summarization",
            "B. Domain expertise",
            "C. No correlating data",
            "D. First principles"
        ],
        answer: [2],
        explanation: "City and State are correlated data, as the provided city always resides in the same state. If we keep these two inputs separate in a model, it will lead to correlation errors."
    },
    {
        id: "ite303c_043",
        question: "Once a dataset has been cleaned, which accuracy guideline ensures your model is looking at the problem correctly?",
        options: [
            "A. First principles",
            "B. Objective summarization",
            "C. Domain expertise",
            "D. Dataset verification"
        ],
        answer: [2],
        explanation: "It's important to incorporate domain expertise in model creation, to ensure that we are approaching the problem correctly"
    },
    {
        id: "ite303c_044",
        question: "A good example of cultural reflection in training data is",
        options: [
            "A. a predictive model incorporates training data from a variety of sources",
            "B. a model fails to recognize cultural differences due to incorrect attributes",
            "C. a model selects for one demographic less often because of their historical representation",
            "D. an image recognition model selects one face over another based on sample data"
        ],
        answer: [2],
        explanation: "Cultural reflection of training data refers to the concept of \"data as a mirror\" - meaning that any datasets carry societal and cultural influences from the people they are collected from"
    },
    {
        id: "ite303c_045",
        question: "A good example of empirical reflection in training data is",
        options: [
            "A. A true positive result that defies the training data set",
            "B. an image recognition model cannot tell a difference between a photo of a dog and a photo of a photo of a dog",
            "C. an image recognition model selects one face over another based on sample data",
            "D. a model fails to recognize cultural differences due to incorrect attributes"
        ],
        answer: [1],
        explanation: "Cultural reflection of training data refers to the concept of \"data as a mirror\" - meaning that any datasets carry societal and cultural influences from the people they are collected from"
    },
    {
        id: "ite303c_046",
        question: "A training set based on feeding 60% of data, validating on 20% of data, and then designing multiple tests for the remaining 20% of data is referred to as an",
        options: [
            "A. optimized training set",
            "B. predictive training set",
            "C. false positive set",
            "D. classic training set"
        ],
        answer: [0],
        explanation: "A classic training set relies on running a final accuracy test before deploying a model. An optimized training set relies on multiple tests to ensure that a model is free of bias"
    },
    {
        id: "ite303c_047",
        question: "Our goals for building an ethical predictive model include making sure the results are",
        options: [
            "A. accurate, fair and explainable",
            "B. precise, explainable, predictable",
            "C. accurate, precise, fair",
            "D. precise, methodical, ethical"
        ],
        answer: [0],
        explanation: "An ethical predictive model needs to go beyond the typical guideline for accuracy. We need it to be fair (adjusting for potential blind spots/bias) and explainable (avoiding the black box problem)"
    },
    {
        id: "ite303c_048",
        question: "Unknown Unknowns refer to",
        options: [
            "A. an uncertainty of how the data is gathered",
            "B. facing unknown empirical data with an incomplete dataset",
            "C. being unsure about the morals of the research team",
            "D. lack of explainability and what a model is actually looking at to make it's prediction"
        ],
        answer: [3],
        explanation: "An unknown unknown refers to the high potential for a model to use unexpected attributes in its predictions"
    },
    {
        id: "ite303c_049",
        question: "Narrow AI (ANI) is defined as a specific type of artificial intelligence in which a technology outperforms humans in some defined task.",
        options: [
            "A. true",
            "B. false"
        ],
        answer: [0],
        explanation: "This is true. Narrow intelligence can succeed in a specific task, but cannot apply those algorithms to a separate task"
    },
    {
        id: "ite303c_050",
        question: "An ethical, evolved predictive model would need to mimic a researcher's ability to _________________",
        options: [
            "A. eliminate bias",
            "B. self-learn",
            "C. scrub data",
            "D. parse through datasets"
        ],
        answer: [0],
        explanation: "An evolved predictive model would need to mimic a researcher's ability to analyze datasets, eliminating bias and leaning on domain expertise"
    },
    {
        id: "ite303c_051",
        question: "The second evolution of decision-making AI would enable",
        options: [
            "A. predictive models to start companies",
            "B. predictive models to decide war strategy",
            "C. predictive models to drive cars",
            "D. predictive models to approve loans"
        ],
        answer: [1],
        explanation: "When governments begin relying on AI in second-evolution decision making models, researchers will need to program a moral code into that model to make decisions in accordance to an agreed-upon universal ethical standard"
    },
    {
        id: "ite303c_052",
        question: "Researchers believe that a general-purpose AI must be available to as many as possible, making it similar to a ________",
        options: [
            "A. government program",
            "B. system of money",
            "C. utility",
            "D. tax plan"
        ],
        answer: [2],
        explanation: "Researchers believe that a general purpose AI must be available to as many as possible, making it similar to a utility like electricity."
    },
    {
        id: "ite303c_053",
        question: "A perverse instantiation is an unintended negative outcome of programming a goal that is too specific given to general intelligence",
        options: [
            "A. true",
            "B. false"
        ],
        answer: [1],
        explanation: "This is false. When giving AI a broad goal like \"ensure happiness for all\", the AI could likely find an overall negative outcome that still suits that goal, such as plugging all humans into a simulated world"
    },
    {
        id: "ite303c_054",
        question: "For-profit colleges tend to use predictive models",
        options: [
            "A. to evaluate the standards of their professors",
            "B. to accelerate their research departments",
            "C. to see which applicants are most likely to graduate",
            "D. to see which candidates are most likely to receive government loans."
        ],
        answer: [3],
        explanation: "For-profit college models predict which candidates are most likely to receive government loans. The consequence is that they end up targeting vulnerable groups like veterans, who are more likely to receive but also default on those loans"
    },
    {
        id: "ite303c_055",
        question: "For a model to clean, parse, and self-train it's own dataset while remaining impartial, the model needs",
        options: [
            "A. more powerful computing algorithms to auto-scrub data",
            "B. a list of bias and domain tests to run and adjust for",
            "C. a test for recency bias",
            "D. 10x the amount of data"
        ],
        answer: [1],
        explanation: "An evolved predictive model would need to mimic a researcher's ability to analyze datasets, eliminating bias and leaning on domain expertise"
    },
    {
        id: "ite303c_056",
        question: "For a model to make decisions that involve human life, the model needs",
        options: [
            "A. list of bias tests to run against possible wrong outcomes",
            "B. enough computing power to make correct predictions 100% of the time",
            "C. programmed reflexive decision making ability",
            "D. a moral code of reasoning and priorities"
        ],
        answer: [3],
        explanation: "When evolving a model to make decisions that affect human lives, researchers will need to program a moral code into that model to make decisions in accordance with an agreed-upon universal ethical standard"
    },
    {
        id: "ite303c_057",
        question: "A type of artificial intelligence that outperforms humans in some defined task is known as",
        options: [
            "A. Special AI",
            "B. General AI",
            "C. Narrow AI",
            "D. AEI"
        ],
        answer: [2],
        explanation: "Narrow AI (ANI) is defined as \"a specific type of artificial intelligence in which a technology outperforms humans in some very narrowly defined task.\""
    },
    {
        id: "ite303c_058",
        question: "A type of artificial intelligence that outperforms humans in all tasks is known as",
        options: [
            "A. Outwit AI",
            "B. General AI",
            "C. Specific AI",
            "D. Encompassing AI"
        ],
        answer: [1],
        explanation: "General AI (AGI) is \"the hypothetical intelligence of a machine that has the capacity to understand or learn any intellectual task that a human being can.\""
    },
    {
        id: "ite303c_059",
        question: "In 2019, ____% of equity-futures and cash-equity trades were executed by algorithms",
        options: [
            "A. 20-30%",
            "B. 1-5%",
            "C. 11-17%",
            "D. 80-90%"
        ],
        answer: [3],
        explanation: "According to Deutsche Bank, 90% of equity-futures trades and 80% of cash-equity trades are executed by algorithms without any human input."
    },
    {
        id: "ite303c_060",
        question: "The optimistic view of general AI could be accurately summarized as AI as a ____",
        options: [
            "A. utility",
            "B. human right",
            "C. weapon",
            "D. peace-keeping tool"
        ],
        answer: [0],
        explanation: "The optimist view is of AI as a similar concept to electricity: one provider of algorithms, every company gets to utilize them equally"
    },
    {
        id: "ite303c_061",
        question: "The pessimist view of general AI references a scenario in which advancement is _____",
        options: [
            "A. winner take all",
            "B. a potential extinction event",
            "C. impossible",
            "D. creating AI for all governments"
        ],
        answer: [0],
        explanation: "The viewpoint of pessimists in the AI community is that the first to utilize a general-purpose AI will be able to outcompete and overtake all others in the industry, due to the speed of learning seen by AGI"
    },
    {
        id: "ite303c_062",
        question: "An ethical general purpose AI must _____ while not harming the safety of humanity",
        options: [
            "A. keep those in power responsible",
            "B. be 100% accurate",
            "C. not enact hate",
            "D. benefit as many people as possible"
        ],
        answer: [3],
        explanation: "Researchers believe that a general-purpose AI must be available to everyone equally, to avoid those in power using algorithms to exploit those not in power"
    },
    {
        id: "ite303c_063",
        question: "\"Companies have an obligation to their shareholders\" is part of a view that sees artificial intelligence as",
        options: [
            "A. a gimmick for enterprises, unless general intelligence is achieved",
            "B. an overall good for humanity, no matter the consequences",
            "C. a harmful tool that will bring about the end of capitalism",
            "D. just another tool that accelerates research, like online advertising"
        ],
        answer: [3],
        explanation: "Those in the business community exploiting predictive modeling for profits believe AI is just another tool that accelerates research and development"
    },
    {
        id: "ite303c_064",
        question: "An unintended negative outcome of programming a broad goal into general intelligence is known as",
        options: [
            "A. an enduring output",
            "B. perverse instantiation",
            "C. artificial sanctification",
            "D. an ethical dilemma"
        ],
        answer: [1],
        explanation: "When giving AI a broad goal like \"ensure happiness for all\", the AI could likely find an overall negative outcome that still suits that goal, such as plugging all humans into a simulated world"
    },
    {
        id: "ite303c_065",
        question: "True or false: The definition of fairness is \"just treatment without bias and contempt\"",
        options: [
            "A. true",
            "B. false"
        ],
        answer: [1],
        explanation: "This is false, since potential contempt does not matter in fairness. We define fairness as \"impartial and just treatment or behavior without favoritism or discrimination.\""
    },
    {
        id: "ite303c_066",
        question: "Statistical parity as a fairness goal makes the most sense when",
        options: [
            "A. distributing randomly, ex. tickets",
            "B. distributing by merit, ex. loans",
            "C. distributing by gender, ex. tickets",
            "D. distributing by error rate, ex. loans"
        ],
        answer: [0],
        explanation: "Statistical parity in machine learning means that each group received the same output. In ticket distribution, this means group A would receive 50% of tickets and group B would receive the other 50%"
    },
    {
        id: "ite303c_067",
        question: "Error rate parity means an equal chance of",
        options: [
            "A. outcomes for each group",
            "B. prediction rate for each group",
            "C. approval for each group",
            "D. mistakes made for each group"
        ],
        answer: [3],
        explanation: "Error rate parity means the model has the same rate of making mistakes for each group, increasing fairness on the outputs"
    },
    {
        id: "ite303c_068",
        question: "If you know one group is misrepresented in merit by training data, one way to ensure fairness is to",
        options: [
            "A. protect a different group",
            "B. reboot the training data",
            "C. delete that group's data from the dataset",
            "D. create a separate threshold for that group"
        ],
        answer: [3],
        explanation: "If you know one group is misrepresented in merit by training data, due to a one-time event that won't affect the actual accuracy, one way to ensure fairness is to give that group a different threshold for approval by the algorithm"
    },
    {
        id: "ite303c_069",
        question: "True or false: fairness in machine learning cannot protect all individuals within protected groups from harm",
        options: [
            "A. true",
            "B. false"
        ],
        answer: [0],
        explanation: "This is true. It's impossible to build a model that is accurate while protecting each individual input. Therefore, our goal is to maximize the number of possible protected groups, while recognizing it will be imperfect in nature"
    },
    {
        id: "ite303c_070",
        question: "Our goal in machine learning fairness is to minimize _______ as long as _______ is obtained",
        options: [
            "A. accuracy issues, unfairness",
            "B. error rates, parity",
            "C. unfairness, equality",
            "D. equality, error rates"
        ],
        answer: [1],
        explanation: "Our goal in machine learning fairness is to minimize error rates (accuracy) as long as parity (fairness) is obtained"
    },
    {
        id: "ite303c_071",
        question: "Fairness is best defined as just treatment without __________",
        options: [
            "A. bias and contempt",
            "B. favoritism or discrimination",
            "C. discrimination and prejudice",
            "D. prejudice and favoritism"
        ],
        answer: [1],
        explanation: "Fairness: \"impartial and just treatment or behavior without favoritism or discrimination.\""
    },
    {
        id: "ite303c_072",
        question: "Which type of fairness would make sense when dividing tickets evenly between groups?",
        options: [
            "A. statistical parity",
            "B. equality of false positives",
            "C. equality of prediction rate",
            "D. error rate parity"
        ],
        answer: [0],
        explanation: "statistical parity in machine learning means that each group received the same output. In ticket distribution, this means group A would receive 50% of tickets and group B would receive the other 50%"
    },
    {
        id: "ite303c_073",
        question: "Which type of fairness fails to address merit while maintaining accuracy?",
        options: [
            "A. equality of false positives",
            "B. error rate parity",
            "C. statistical parity",
            "D. equality of prediction rate"
        ],
        answer: [2],
        explanation: "When judging for merit, when some groups will be more qualified than others, statistical parity will still distribute evenly, but at the expense of accuracy"
    },
    {
        id: "ite303c_074",
        question: "A model that prioritizes equality term-69on the outputs uses",
        options: [
            "A. equality of prediction rate",
            "B. equality of assignment rate",
            "C. statistical parity",
            "D. error rate parity"
        ],
        answer: [3],
        explanation: "A model that optimizes for equality on the output uses error rate parity. This means that the model has the same rate of making mistakes for each group, increasing fairness on the outputs"
    },
    {
        id: "ite303c_075",
        question: "Fairness in machine learning can protect groups from bias, but can still harm",
        options: [
            "A. individuals within those groups",
            "B. future models",
            "C. researchers",
            "D. training datasets"
        ],
        answer: [0],
        explanation: "It's impossible to build a model that is accurate while protecting each individual input. Therefore, our goal is to maximize the number of possible protected groups, while recognizing it will be imperfect in nature"
    },
    {
        id: "ite303c_076",
        question: "A goal of a fair model's accuracy standards is to",
        options: [
            "A. minimize the error rate as long as parity is obtained",
            "B. minimize the fairness score as long as the error rate isn't affected",
            "C. minimize the error rate as long as the training data isn't affected",
            "D. minimize the quality metrics as long as the quantity metrics aren't affected"
        ],
        answer: [0],
        explanation: "A fair predictive model will optimize for both accuracy and fairness: it must reflect the training data and minimize error rates as long as parity is obtained"
    },
    {
        id: "ite303c_077",
        question: "A model that makes more mistakes by moving its decision threshold down 40% of its worthiness metric will be potentially",
        options: [
            "A. less fair but more accurate",
            "B. less accurate and less fair",
            "C. fairer but less accurate",
            "D. more accurate and fairer"
        ],
        answer: [2],
        explanation: "Fairness and accuracy are usually at odds in predictive modeling. Thus a model that becomes less accurate by moving down its decision threshold will be fairer in nature, allowing more people of different groups through"
    },
    {
        id: "ite303c_078",
        question: "If one group comprises the majority of the training data, they will skew the dataset and give the model",
        options: [
            "A. more confidence about that group",
            "B. less confidence about that group",
            "C. less fairness for that group",
            "D. more fairness for that group"
        ],
        answer: [1],
        explanation: "If one group dominates the training set, the model will be skewed and thus more confident about the decisions it makes about that group"
    },
    {
        id: "ite303c_079",
        question: "If we know one group's worthiness score has been artificially inflated, one solution for fairness is to",
        options: [
            "A. remove that group from the dataset",
            "B. Add the inflation to the other data",
            "C. creating separate decision thresholds for each group",
            "D. balance the error rate by prioritizing the other group"
        ],
        answer: [2],
        explanation: "Creating separate thresholds for model decisions can be a useful method when one group has inflated metrics that don't ultimately affect the outcome"
    },
    {
        id: "ite303c_080",
        question: "An unfair model will by nature",
        options: [
            "A. optimize for making the fewest mistakes",
            "B. try to balance groups automatically",
            "C. optimize for making the fewest decisions",
            "D. optimize for making the most errors"
        ],
        answer: [0],
        explanation: "A unfair model is only concerned with accuracy by it's \"programmed\" nature. Therefore, it will optimize for making the fewest mistakes, leading to potentially biased outcomes"
    },
    {
        id: "ite303c_081",
        question: "True or false: it is practical to protect all possible subgroups in predictive modeling",
        options: [
            "A. true",
            "B. false"
        ],
        answer: [1],
        explanation: "This is false. Because fairness is a direct tradeoff for accuracy, the more subgroups that are protected, the more likely the accuracy score is going to be impractical for a real-world model"
    },
    {
        id: "ite303c_082",
        question: "In machine learning, a pareto curve helps us",
        options: [
            "A. pick an optimal threshold for accuracy and error rate",
            "B. highlight the inequality in our model",
            "C. pick an optimal tradeoff between fairness and accuracy",
            "D. select which model will give the best results"
        ],
        answer: [0],
        explanation: "A pareto efficiency, or curve, helps us to pick an ideal tradeoff between fairness and accuracy. Picking a point that is either above or below the curve means we could select a model that is both more fair and accurate"
    },
    {
        id: "ite303c_083",
        question: "True or false: A blind attribute model protects group fairness by not including group membership in predictions",
        options: [
            "A. true",
            "B. false"
        ],
        answer: [1],
        explanation: "This is false. While a blind attribute model does not include group membership in predictions, it does not protect group fairness because group membership can be identified by other attributes"
    },
    {
        id: "ite303c_084",
        question: "An adversarial algorithm is _______________ to identify weaknesses in black box models",
        options: [
            "A. trained with large datasets",
            "B. purposefully biased",
            "C. purposefully fair",
            "D. given no data"
        ],
        answer: [1],
        explanation: "An adversarial algorithm is purposefully biased, trained on similar data to try to identify where machine learning could be biased. It uses these insights to feed potentially biased inputs into a black box model"
    },
    {
        id: "ite303c_085",
        question: "True or false: our analysis revealed that Word2Vec is not a black box model",
        options: [
            "A. true",
            "B. false"
        ],
        answer: [0],
        explanation: "This is true. Because we know exactly what our theoretical Word2Vec model was trained on, we can more easily replicate its decisions and see where it's biased."
    },
    {
        id: "ite303c_086",
        question: "Which step in the fairness process would be most appropriate to introduce an auditing model?",
        options: [
            "A. in-processing",
            "B. pre-processing",
            "C. post-processing",
            "D. sub-processing"
        ],
        answer: [0],
        explanation: "An auditing model happens after data collection and before fairness scores are generated, making it in in-processing method"
    },
    {
        id: "ite303c_087",
        question: "A state where resources cannot be reallocated to make one individual better off without making at least one individual worse off is known as a",
        options: [
            "A. prisoner's dilemma",
            "B. boron letter",
            "C. aggregate curve",
            "D. pareto efficiency"
        ],
        answer: [3],
        explanation: "A pareto efficiency describes an optimized tradeoff. Its classic definition is a state where resources cannot be reallocated to make one individual better off without making at least one individual worse off"
    },
    {
        id: "ite303c_088",
        question: "In machine learning, what do we plot on the X,Y axis to determine a pareto curve?",
        options: [
            "A. Error rate, true positive rate",
            "B. Rejection rate, false-positive rate",
            "C. Rejection rate, subgroup fairness rate",
            "D. Error rate, rejection rate"
        ],
        answer: [3],
        explanation: "To plot the tradeoffs of each model, we construct an X-axis of increasing error rate and a Y-axis of increasing rejection rate (fairness score)"
    },
    {
        id: "ite303c_089",
        question: "Why is it impractical to protect all possible subgroups in predictive models?",
        options: [
            "A. There won't be enough data to reflect each subgroup",
            "B. Fairness scores won't be high enough to be reasonable",
            "C. Accuracy will be lowered beyond a reasonable rate",
            "D. Individuals do not need protection from predictive models"
        ],
        answer: [2],
        explanation: "Because fairness is a direct tradeoff for accuracy, the more subgroups that are protected, the more likely the accuracy score is going to be impractical for a real-world model"
    },
    {
        id: "ite303c_090",
        question: "A model that equalizes the number of mistakes it makes for each subgroup to reduce harm is deciding on",
        options: [
            "A. equality of training data",
            "B. equality of true outcomes",
            "C. equality of false negatives",
            "D. equality of prediction bias"
        ],
        answer: [2],
        explanation: "A classic definition of harm is a false negative - meaning you were creditworthy but denied credit. Therefore, a model aiming for fairness in harm distributed would equalize the false negatives produced amongst subgroups"
    },
    {
        id: "ite303c_091",
        question: "A _________ model can still be unfair even though it won't explicitly know which groups are being inputted into the system",
        options: [
            "A. single attribute",
            "B. biased training",
            "C. blind attribute",
            "D. false-negative optimized"
        ],
        answer: [2],
        explanation: "A blind attribute model can still be unfair to groups, even though it won't take those groups as inputs. Ex. Amazon's hiring model was blind to gender as an attribute, but still discriminated against women in practice"
    },
    {
        id: "ite303c_092",
        question: "What tools do researchers have to evaluate the fairness of existing black box models?",
        options: [
            "A. Evaluate inputs, evaluate data",
            "B. Change training data, evaluate outputs",
            "C. Change inputs, evaluate outputs",
            "D. Change inputs, evaluate training data"
        ],
        answer: [2],
        explanation: "Evaluating a black box model entails not having access to the training data or insight into how the model makes decisions. The only thing we can control is the inputs and evaluating the outputs"
    },
    {
        id: "ite303c_093",
        question: "A \"purposefully biased\" algorithm used to identify unfair attributes is known as",
        options: [
            "A. a discriminatory algorithm",
            "B. an adversarial algorithm",
            "C. a predictive model",
            "D. an aggregate algorithm"
        ],
        answer: [1],
        explanation: "An adversarial algorithm is trained on similar data to try to identify where machine learning could be biased. It uses these insights to feed potentially biased inputs into a black box model"
    },
    {
        id: "ite303c_094",
        question: "In presenting an audit report, a researcher would",
        options: [
            "A. de-bias the results",
            "B. re-train the model",
            "C. score the weight of input attributes on output",
            "D. prevent the model from launching"
        ],
        answer: [2],
        explanation: "An audit report can vary in scope, but the main benefit is a list of weighted input attributes, all scored by how much they affect the model's decision"
    },
    {
        id: "ite303c_095",
        question: "In fixing the Word2Vec model, we have an advantage over a traditional black box model in that",
        options: [
            "A. we can decide which inputs to use",
            "B. we can see the decision-making model",
            "C. we can generate a fairness score",
            "D. we have access to the training data"
        ],
        answer: [3],
        explanation: "In knowing that the Word2Vec model is trained on a subset of Wikipedia data, we can train our adversarial model on the same training data and have a much better idea of the biases present"
    },
    {
        id: "ite303c_096",
        question: "An auditing model is an example of a ______ bias mitigation method",
        options: [
            "A. post-processing",
            "B. sub-processing",
            "C. pre-processing",
            "D. in-processing"
        ],
        answer: [3],
        explanation: "An auditing model happens after data collection and before fairness scores are generated, making it an in-processing method"
    },
    {
        id: "ite303c_097",
        question: "True or false: the no free lunch theorem states that we cannot have fair models without giving up something else",
        options: [
            "A. true",
            "B. false"
        ],
        answer: [1],
        explanation: "This is false. The no free lunch theorem tells us that all models have the same error rate when averaged over all possible data, meaning that we will have to pick an area to focus in on and other areas to ignore in our modeling"
    },
    {
        id: "ite303c_098",
        question: "Which of the following is a good example of sample bias?",
        options: [
            "A. your model is trained to avoid bias, but it contains no samples of that bias",
            "B. your model is designed to give loans to those who need it, but it is trained with unfair data",
            "C. your model is trained to recognize pets, but you only give it photos of dogs",
            "D. your model is broken because it cannot sample the right attribute"
        ],
        answer: [2],
        explanation: "\"Sample bias refers to the situation when collected data doesn't represent the full environment of the problem. Ex. you build model that identifies subjects in photos, but only train it on daytime photos so it's unable to parse photos taken at night \""
    },
    {
        id: "ite303c_099",
        question: "When cleaning/parsing data removes a potentially important attribute, that is referred to as",
        options: [
            "A. observer bias",
            "B. automation bias",
            "C. confirmation bias",
            "D. exclusion bias"
        ],
        answer: [3],
        explanation: "When cleaning/parsing data removes a potentially important attribute, that is referred to as exclusion bias. We delete some feature(s) thinking that they're irrelevant to our labels/outputs based on pre-existing beliefs."
    },
    {
        id: "ite303c_100",
        question: "Labeling outputs made by predictive models can avoid which feedback issue?",
        options: [
            "A. sample bias",
            "B. predictive loop bias",
            "C. fairness score bias",
            "D. re-training bias"
        ],
        answer: [3],
        explanation: "Re-training bias occurs when data that was either output directly or heavily influence by a predictive model is then used to re-train the model later on. By labeling outputs, we can avoid this issue"
    },
    {
        id: "ite303c_101",
        question: "True or false: Recommendation engines are not as susceptible to feedback bias due to their constraints on data inputs",
        options: [
            "A. true",
            "B. false"
        ],
        answer: [1],
        explanation: "This is false. Recommendation engines are very susceptible to feedback bias due to the short feedback cycles and collaborative filtering they offer. Ex. Netflix recommendations, dating apps"
    },
    {
        id: "ite303c_102",
        question: "A \"best response\" in game theory is when",
        options: [
            "A. a user has no choice but to follow the group dynamic to benefit",
            "B. a user leaves a group setting to pursue individual goals",
            "C. a user chooses the least fair option",
            "D. a user comes up with the best response from the group"
        ],
        answer: [0],
        explanation: "In game theory, a best response is when a user takes in the group dynamic and then is forced to make the best possible individual decision after evaluating the situation"
    },
    {
        id: "ite303c_103",
        question: "The __________ theorem states that all models have the same error rate when averaged over all possible data generating distributions.",
        options: [
            "A. no free lunch",
            "B. biased aggregate",
            "C. zero handouts",
            "D. pareto efficiency"
        ],
        answer: [0],
        explanation: "The no free lunch theorem tells us that all models have the same error rate when averaged over all possible data, meaning that we will have to pick an area to focus in on and other areas to ignore in our modeling"
    },
    {
        id: "ite303c_104",
        question: "As a cognitive bias, humans see lack of context/meaning around a piece of information and tend to",
        options: [
            "A. fill in gaps with existing knowledge",
            "B. use biased sources of research",
            "C. disagree with their previous beliefs",
            "D. seek authority figures"
        ],
        answer: [0],
        explanation: "A general takeaway from cognitive research is that humans tend to fill in gaps with information that lacks context/meaning. We do so to put the new information into contexts that we can better understand"
    },
    {
        id: "ite303c_105",
        question: "When your collected data doesn't accurately reflect the full environment, you're experiencing",
        options: [
            "A. prejudice bias",
            "B. exclusion bias",
            "C. sample bias",
            "D. observer bias"
        ],
        answer: [2],
        explanation: "Sample bias refers to the situation when collected data doesn't represent the full environment of the problem. Ex. you build a model that identifies subjects in photos, but only train it on daytime photos so it's unable to parse photos taken at night"
    },
    {
        id: "ite303c_106",
        question: "The tendency to only seek attributes in existing collected data is known as",
        options: [
            "A. availability bias",
            "B. observer bias",
            "C. exclusion bias",
            "D. prejudice bias"
        ],
        answer: [0],
        explanation: "The tendency to only create attributes with existing data is known as availability bias. Researchers should be aware of when it may be appropriate to return to the data collection stage"
    },
    {
        id: "ite303c_107",
        question: "An example of automation bias is",
        options: [
            "A. using new data over existing data",
            "B. using parsed twitter data over parsed Facebook data",
            "C. using scraped twitter data over survey data",
            "D. using biased survey data instead of parsed survey data"
        ],
        answer: [2],
        explanation: "Automation bias is the tendency to favor data sources that were automatically generated"
    },
    {
        id: "ite303c_108",
        question: "A ________ is when a model is validated by it's own influence on predictions",
        options: [
            "A. self-fulfilling prediction",
            "B. dataset scrub",
            "C. training set error",
            "D. false prediction set"
        ],
        answer: [0],
        explanation: "When a model validates mainly through its own influence on predictions, it's called a self-fulfilling prediction. Ex. a crime model sends police to a neighborhood, that increases arrests there and validates the model"
    },
    {
        id: "ite303c_109",
        question: "One way to avoid feedback loops in machine learning is to",
        options: [
            "A. destroy previous training data",
            "B. label outputs to prevent re-training bias",
            "C. scrub datasets after each decision",
            "D. investigate exclusion bias"
        ],
        answer: [1],
        explanation: "By labeling outputs that came from the predictive model, we can avoid that data being weighted similarly in re-training to avoid a feedback loop"
    },
    {
        id: "ite303c_110",
        question: "Dating algorithms become biased mostly through offering users _________",
        options: [
            "A. collaborative filtering",
            "B. different ways to match with users",
            "C. access to separate data models",
            "D. unlimited matches per day"
        ],
        answer: [0],
        explanation: "By allowing users to collaboratively filter their preferences, dating algorithms become biased as the game theory mechanics force irrational decisions to be made"
    },
    {
        id: "ite303c_111",
        question: "Predictive loops in marketplace models like dating apps are especially susceptible to bias due to",
        options: [
            "A. short feedback cycles",
            "B. fairness quotients",
            "C. marketplace forces",
            "D. engagement levels"
        ],
        answer: [0],
        explanation: "Because each user action can potentially affect the dataset and model, the short feedback cycle in dating apps can make biased outcomes appear very quickly"
    },
    {
        id: "ite303c_112",
        question: "Game theory states that outcomes that are best for _____ can be obscured by outcomes best for ______",
        options: [
            "A. the group, the dataset",
            "B. the model, the individual",
            "C. the dataset, the model",
            "D. the group, the individual"
        ],
        answer: [3],
        explanation: "Dating apps are a great example of game theory. Because each user cannot see the actions of the group, outcomes that are best for the group are obscured by outcomes best for the individual. Therefore, ideal outcomes are rare"
    },
    {
        id: "ite303c_113",
        question: "A dataset column that cannot directly identify, like zip code, is called a",
        options: [
            "A. quasi-identifiable column",
            "B. personally identifiable column",
            "C. sensitive column",
            "D. non-sensitive column"
        ],
        answer: [0],
        explanation: ""
    },
    {
        id: "ite303c_114",
        question: "True or false: k-anonymity protects users from all privacy violations",
        options: [
            "A. true",
            "B. false"
        ],
        answer: [1],
        explanation: "This is false. While k-anonymity can protect individuals in a dataset, it cannot protect against harms that come from group inclusion, or from re-identification through multiple datasets"
    },
    {
        id: "ite303c_115",
        question: "Which of the following is an example of partially obfuscated data?",
        options: [
            "A. Had a stroke",
            "B. 94103",
            "C. 65 years old",
            "D. 60-70 years old"
        ],
        answer: [3],
        explanation: "This answer is correct because it is quasi-identifiable information that is partially obfuscated"
    },
    {
        id: "ite303c_116",
        question: "True or false: a non-sensitive column can endanger user privacy",
        options: [
            "A. True",
            "B. false"
        ],
        answer: [0],
        explanation: "This is true. While a column may seem non-sensitive today, advances in algorithmic re-identification can render the column sensitive or even personally identifiable in the future"
    },
    {
        id: "ite303c_117",
        question: "Why is it important to limit precise outputs in a predictive model?",
        options: [
            "A. It can help users make better decisions",
            "B. It can help limit adversarial attacks",
            "C. It limits the ability of researchers",
            "D. It removes the datasets in danger of violating privacy"
        ],
        answer: [1],
        explanation: "Limiting precise outputs (Yes/No vs 64%) helps protect a predictive model against adversarial attacks. When inputs are modified, the outputs will reveal less information about how the model works and the dataset underneath"
    },
    {
        id: "ite303c_118",
        question: "Where is a perturbed input used?",
        options: [
            "A. To observe differing outputs",
            "B. To identify an error in a machine learning model",
            "C. To increase public dataset awareness",
            "D. When building a model"
        ],
        answer: [0],
        explanation: "The process of perturbing inputs is how an adversarial model gains insights into how an algorithm works. By making small changes to the inputs, the observed outputs can highlight"
    },
    {
        id: "ite303c_119",
        question: "A dataset attribute that is not identifiable but constitutes data about the individual that needs to be protected is known as a",
        options: [
            "A. explicitly private column",
            "B. non-sensitive column",
            "C. quasi-identifier",
            "D. sensitive column"
        ],
        answer: [3],
        explanation: "A sensitive column is a column in a dataset that needs privacy protection despite not being directly identifiable"
    },
    {
        id: "ite303c_120",
        question: "K-anonymity in a dataset is achieved when each individual cannot be",
        options: [
            "A. hidden from a quasi-identifier column as long as K individuals belong",
            "B. harmed from datasets with K individuals belonging to the sensitive class",
            "C. distinguished from at least K individuals who are also in the dataset",
            "D. reidentified in K datasets"
        ],
        answer: [2],
        explanation: "Property of a dataset where each individual cannot be distinguished from at least K individuals who are also in the dataset"
    },
    {
        id: "ite303c_121",
        question: "A major downside to k-anonymity is that re-identification is possible with",
        options: [
            "A. database leaks",
            "B. expanding k values",
            "C. multiple datasets",
            "D. sensitive columns"
        ],
        answer: [2],
        explanation: "k-anonymity can preserve privacy well in a single dataset, but with multiple datasets available, it becomes much harder to protect privacy, as column attributes can be combined to re-identify an individual"
    },
    {
        id: "ite303c_122",
        question: "A hospital dataset protects whether an individual has had either a stroke, heart attack, or staph infection. The individual may still be harmed via dataset",
        options: [
            "A. k-anonymity",
            "B. privacy columns",
            "C. group inclusion",
            "D. security issues"
        ],
        answer: [2],
        explanation: "Harm via group inclusion is possible even if a sensitive column is protected. In this example, even though the specific condition is not known, the individual could be denied insurance by knowing that they have at least one of the conditions"
    },
    {
        id: "ite303c_123",
        question: "A non-sensitive column may become sensitive or even identifiable when viewed through the lens of",
        options: [
            "A. human privacy violations",
            "B. societal privacy violations",
            "C. algorithmic privacy violations",
            "D. community privacy violations"
        ],
        answer: [2],
        explanation: "An algorithmic privacy violation refers to the power of machine learning to make predictions and assessments from previously non-sensitive data"
    },
    {
        id: "ite303c_124",
        question: "The Netflix prize privacy scandal is an example of reidentification through",
        options: [
            "A. multiple datasets",
            "B. sensitive columns",
            "C. database leaks",
            "D. k-anonymity"
        ],
        answer: [0],
        explanation: "The Netflix prize dataset, while individual records were anonymized, contained privacy violations as soon as it was combined with other public datasets, like IMDB user ratings"
    },
    {
        id: "ite303c_125",
        question: "An example of a public dataset at risk of an algorithmic privacy violation is the",
        options: [
            "A. Google search view dataset",
            "B. Fidelity Bank lending dataset",
            "C. Stanford hospital dataset",
            "D. Google Maps satellite view dataset"
        ],
        answer: [3],
        explanation: "Researchers have shown that insurance companies could use Google Maps satellite view data to predict risk for their customers. Because this is a public dataset, it's an example of an algorithmic privacy violation"
    },
    {
        id: "ite303c_126",
        question: "The nature of a predictive model may reveal ___________",
        options: [
            "A. the ethics of the individual inputs",
            "B. the data it is trained on",
            "C. the researchers behind it",
            "D. the algorithm's bias"
        ],
        answer: [1],
        explanation: "By changing and observing inputs and reading outputs, a model can reveal some aspects of the data it's trained on"
    },
    {
        id: "ite303c_127",
        question: "An adversarial model relies on using __________ to observe different outputs",
        options: [
            "A. perturbed inputs",
            "B. algorithm detection",
            "C. sensitive columns",
            "D. pressure inputs"
        ],
        answer: [0],
        explanation: "The process of perturbing inputs is how an adversarial model gains insights into how an algorithm works. By making small changes to the inputs, the observed outputs can highlight"
    },
    {
        id: "ite303c_128",
        question: "One way to counter a potential adversarial algorithm is by",
        options: [
            "A. changing the datasets",
            "B. improving model transparency",
            "C. limiting precise outputs",
            "D. banning model updates"
        ],
        answer: [2],
        explanation: "An adversarial algorithm relies on output information to gain insight into the dataset. By outputting less precise information, it becomes harder to reverse engineer the model. Ex. display Pass/Fail vs 62%"
    },
    {
        id: "ite303c_129",
        question: "Which of the following is not a recommended security practice in machine learning?",
        options: [
            "A. Perform threat modeling",
            "B. Enact a sound data governance structure",
            "C. Create a chain of command",
            "D. Ensure that all team members/stakeholders have a basic understanding of security"
        ],
        answer: [2],
        explanation: "The three recommended security practices in machine learning are: Ensure that all team members/stakeholders have a basic understanding of security, enact a sound data governance structure, and perform threat modeling"
    },
    {
        id: "ite303c_130",
        question: "True or false: data minimization is the concept of shrinking datasets to only what is required to fulfill a general purpose",
        options: [
            "A. True",
            "B. False"
        ],
        answer: [1],
        explanation: "This is false. Data minimization is the concept of shrinking datasets to only what is required to fulfill a specific purpose"
    },
    {
        id: "ite303c_131",
        question: "A model working at 95% with 100k rows of data and 97% with 500k rows is an example of",
        options: [
            "A. diminishing returns",
            "B. accuracy issues",
            "C. sensitive columns",
            "D. dataset limitations"
        ],
        answer: [0],
        explanation: "It's important to study diminishing returns of model performance when it relates to privacy. In this example, an extra 2% gain in performance is likely not worth potentially exposing 400k rows of sensitive data"
    },
    {
        id: "ite303c_132",
        question: "True or false: global differential privacy is added after data is collected",
        options: [
            "A. True",
            "B. False"
        ],
        answer: [0],
        explanation: "This is true. Global differential privacy is algorithmic noise added after data is collected, whereas local privacy is noise added at the time of collection"
    },
    {
        id: "ite303c_133",
        question: "Randomized response in local differential privacy gives its users which of the following?",
        options: [
            "A. plausible deniability",
            "B. global privacy",
            "C. k-anonymity",
            "D. fairness guarantees"
        ],
        answer: [0],
        explanation: "An advantage of differential privacy is giving users in the dataset plausible deniability - they can claim that a sensitive datapoint was randomized by the process"
    },
    {
        id: "ite303c_134",
        question: "In reverse-engineering a double coin flip differential model, what would be the amount of falsified \"yes/no\" responses in our dataset?",
        options: [
            "A. 50%",
            "B. 75%",
            "C. 100%",
            "D. 25%"
        ],
        answer: [3],
        explanation: "In a double coin flip model, if heads, the respondent tells the true answer. If tails, they flip again for a random answer. 50% for true, and 25% of the time their random answer will be the correct one (yes/no). So 75% of responses will be truthful, and 25% will be falsified"
    },
    {
        id: "ite303c_135",
        question: "Which of the following is a recommended security practice for machine learning datasets?",
        options: [
            "A. Ensure your team is full of different perspectives",
            "B. Create a chain of command",
            "C. Enact a sound data governance structure",
            "D. Perform threat modeling with beneficial algorithms"
        ],
        answer: [2],
        explanation: "Putting a sound data governance structure in place is one of the best methods to protect the security of a machine learning project"
    },
    {
        id: "ite303c_136",
        question: "The Data minimization principle requires that you limit data collection to only what is __________",
        options: [
            "A. required to fulfill a specific purpose",
            "B. necessary for differential privacy",
            "C. optional for a chain of command",
            "D. needed to have k-anonymity"
        ],
        answer: [0],
        explanation: "Data collected only to what is required to fulfill a specific purpose is what constitutes the data minimization principle"
    },
    {
        id: "ite303c_137",
        question: "Delete unused data __________ is a method of data minimization",
        options: [
            "A. before threat modeling",
            "B. after fairness preparations",
            "C. before modeling",
            "D. early and often"
        ],
        answer: [3],
        explanation: "One way to practice data minimization is to delete unused data early and often, making sure it is truly deleted and not accessible to an attacker"
    },
    {
        id: "ite303c_138",
        question: "GDPR states that \"Personal data shall be adequate, relevant and __________ in relation to the purpose or purposes for which they are processed.\"",
        options: [
            "A. not excessive",
            "B. broadly applicable",
            "C. highly specified",
            "D. thoroughly vetted"
        ],
        answer: [0],
        explanation: "GDPR introduces subjective language about what is excessive when it comes to personal identifying data. This is important to note as you begin a machine learning project"
    },
    {
        id: "ite303c_139",
        question: "What can be learned from a predictive model should not change if the _________ is either included or excluded in the training set",
        options: [
            "A. model fairness score",
            "B. dataset filter",
            "C. biased dataset",
            "D. individual's data"
        ],
        answer: [3],
        explanation: "If an individual's inclusion in a dataset cannot change the model, it means that the model cannot be reverse engineered to re-identify them as easily"
    },
    {
        id: "ite303c_140",
        question: "Differential privacy works by adding what to a dataset?",
        options: [
            "A. k-anonymity",
            "B. fairness scoring",
            "C. noise",
            "D. filters"
        ],
        answer: [2],
        explanation: "Differential privacy adds noise to a dataset in an effort to make it harder to reidentify an individual"
    },
    {
        id: "ite303c_141",
        question: "If a coin is flipped, which of the following would ensure \"yes/no\" data is private while still remaining useful?",
        options: [
            "A. heads for true answer, tails for random answer",
            "B. heads for random answer, tails for yes",
            "C. heads for yes, tails for no",
            "D. heads for fake answer, tails for true answer"
        ],
        answer: [0],
        explanation: "In a differential privacy survey, one way to ensure the data is still useful is to flip a coin. If heads, the respondent tells the true answer. If tails, they flip again for a random answer."
    },
    {
        id: "ite303c_142",
        question: "At which level of differential privacy is the outcome secured from even the people collecting answers?",
        options: [
            "A. premium",
            "B. global",
            "C. local",
            "D. k-anonymous"
        ],
        answer: [2],
        explanation: "Local differential privacy is at the level of data collection. This way, those surveyed are protected even from the person collecting the data"
    },
    {
        id: "ite303c_143",
        question: "Plausible deniability refers to the ability of the individual to ______",
        options: [
            "A. doubt the results of a study",
            "B. remove themselves from a machine learning dataset",
            "C. express their participation in a study",
            "D. claim their score was randomized response"
        ],
        answer: [3],
        explanation: "An advantage of differential privacy is giving users in the dataset plausible deniability - they can claim that a sensitive datapoint was randomized by the process"
    },
    {
        id: "ite303c_144",
        question: "In reverse-engineering a double coin flip differential model, what would be the amount of truthful \"yes/no\" responses in our dataset?",
        options: [
            "A. 50%",
            "B. 100%",
            "C. 75%",
            "D. 25%"
        ],
        answer: [2],
        explanation: "In a double coin flip model, if heads, the respondent tells the true answer. If tails, they flip again for a random answer. 50% for true, and 25% of the time their random answer will be the correct one (yes/no). So 75% of responses will be truthful."
    },
    {
        id: "ite303c_145",
        question: "True or false: a glass box model is traditionally preferred by businesses",
        options: [
            "A. True",
            "B. False"
        ],
        answer: [1],
        explanation: "This is false. Businesses have traditionally turned to black box models in part to protect a competitive advantage"
    },
    {
        id: "ite303c_146",
        question: "An ethical model will be more fair and explainable, at the expense of ________",
        options: [
            "A. accuracy",
            "B. privacy",
            "C. recruiting",
            "D. dataset collection"
        ],
        answer: [0],
        explanation: "Ethical models tend to be less accurate, in a tradeoff for fairness, explainability and privacy"
    },
    {
        id: "ite303c_147",
        question: "Explainable models can provide _________, which look very similar to auditing models",
        options: [
            "A. attribute weight charts",
            "B. dataset fairness graphs",
            "C. model renderings",
            "D. privacy protection"
        ],
        answer: [0],
        explanation: "Attribute weight charts are used in explainable models to help the model describe how it made a decision. This usually looks like a bar graph of different attributes with how the model valued them"
    },
    {
        id: "ite303c_148",
        question: "Which of the following is a strategy to improve group outcomes using game theory?",
        options: [
            "A. privacy-enforcing algorithms",
            "B. fairness-enforcing methods",
            "C. explainable recommendation systems",
            "D. ethical predictive sets"
        ],
        answer: [2],
        explanation: "Explainable recommendation engines can tell users why this item was recommended, helping introduce transparency into a typically black box process. This can positively affect group outcomes in dating or navigation apps"
    },
    {
        id: "ite303c_149",
        question: "True or false: glass box models allow agents to re-run them",
        options: [
            "A. True",
            "B. False"
        ],
        answer: [0],
        explanation: "This is true. A glass-box model may contain a \"shut off switch\" for certain attributes, making it easier for human agents to identify when the model is not functioning fairly and re-run it to fix the results"
    },
    {
        id: "ite303c_150",
        question: "A differentially private search algorithm would add some ______ at the expense of _______",
        options: [
            "A. data points, privacy",
            "B. noise, accuracy",
            "C. noise, privacy",
            "D. accuracy, noise"
        ],
        answer: [1],
        explanation: "To add differential privacy, we'd include noise into a user's search profile, so that they'd have plausible deniability if the dataset was breached. This comes at the expense of accuracy"
    },
    {
        id: "ite303c_151",
        question: "Ethical models are _________",
        options: [
            "A. precise, explainable, and private",
            "B. accurate, methodical, and fair",
            "C. precise, explainable, and fair",
            "D. accurate, explainable, and fair"
        ],
        answer: [3],
        explanation: "Ethical models are described as still accurate (useful) while being explainable (transparent) and fair (free of bias)"
    },
    {
        id: "ite303c_152",
        question: "A _____ model is preferred by businesses because they see less competition as a benefit",
        options: [
            "A. ethical",
            "B. precise",
            "C. black box",
            "D. fair"
        ],
        answer: [2],
        explanation: "Businesses turn to black box models in part to protect a competitive advantage"
    },
    {
        id: "ite303c_153",
        question: "One benefit of an explainable model is",
        options: [
            "A. recruiting leverage",
            "B. less competition",
            "C. privacy is preserved",
            "D. the model is more accurate"
        ],
        answer: [0],
        explanation: "Teams that increase transparency with explainable models are more likely to attract top talent working in the machine learning field, leading to recruiting leverage"
    },
    {
        id: "ite303c_154",
        question: "The Strava dataset example illustrates that while differential privacy can protect individuals, it can still harm _______",
        options: [
            "A. groups",
            "B. researchers",
            "C. certain individuals",
            "D. those outside the dataset"
        ],
        answer: [0],
        explanation: "While differential privacy protects the individuals, datasets can still reveal trends or patterns that harm the aggregate or certain groups"
    },
    {
        id: "ite303c_155",
        question: "The explainable AI movement states that cooperation between agents, in this case, algorithms and humans, depends on which of the following?",
        options: [
            "A. competition",
            "B. privacy",
            "C. trust",
            "D. human rights"
        ],
        answer: [2],
        explanation: "The explainable AI movement argues that in order to work together, transparent models are needed to build trust with human agents"
    },
    {
        id: "ite303c_156",
        question: "A benefit of glass-box models is that if an attribute is skewing the fairness of a decision, a human agent may choose to",
        options: [
            "A. alert the team",
            "B. ignore it",
            "C. trust it",
            "D. re-run the algorithm without it"
        ],
        answer: [3],
        explanation: "A glass-box model may contain a \"shut off switch\" for certain attributes, making it easier for human agents to identify when the model is not functioning fairly and adjusting results accordingly"
    },
    {
        id: "ite303c_157",
        question: "_____ algorithms are a challenge to explainable AI, as their complexity makes it difficult to weigh attribute importance",
        options: [
            "A. deep learning",
            "B. malicious",
            "C. ethical",
            "D. explainable"
        ],
        answer: [0],
        explanation: "Due to the complexity of deep learning algorithms, it is more difficult to judge attribute weight and uncover biases"
    },
    {
        id: "ite303c_158",
        question: "When used in recommendation engines, explainable algorithms can help answer the question of ____",
        options: [
            "A. what?",
            "B. who?",
            "C. why?",
            "D. how?"
        ],
        answer: [2],
        explanation: "Explainable recommendation engines can tell users why this item was recommended, helping introduce transparency into a typically black box process"
    },
    {
        id: "ite303c_159",
        question: "Group outcomes being improved through explainable recommendation systems is an example of using _______ to modify outcomes",
        options: [
            "A. government",
            "B. competition",
            "C. privacy",
            "D. game theory"
        ],
        answer: [3],
        explanation: "Game theory describes how researchers can improve the overall health of group outcomes by modifying individual decisions, in this example, with explainable algorithms"
    },
    {
        id: "ite303c_160",
        question: "A major limitation of using explainable, adjustable algorithms is that users tend to make _____ decisions",
        options: [
            "A. selfish",
            "B. private",
            "C. malicious",
            "D. socially conscious"
        ],
        answer: [0],
        explanation: "In a private setting, users are more likely to make selfish decisions, even when it can improve the outcome for other app users"
    },
    {
        id: "ite303c_161",
        question: "Which of the following are important elements of the data minimization principle? (Select two.)",
        options: [
            "A. Only keep data for as long as it is needed",
            "B. Only compress data that needs to be kept as small as possible",
            "C. Only collect data that is strictly necessary",
            "D. Only delete data that can be easily replaced"
        ],
        answer: [0, 2],
        explanation: ""
    },
    {
        id: "ite303c_162",
        question: "Which of the following are logical arguments in favor of an organization maintaining compliance? (Select two)",
        options: [
            "A. Reduced costs of development",
            "B. Reduced time to deployment",
            "C. Avoidance of reputational damage",
            "D. Long-term cost savings due to avoiding fines"
        ],
        answer: [2, 3],
        explanation: ""
    },
    {
        id: "ite303c_163",
        question: "How does increasing AI performance often conflict with the desire for explainability?",
        options: [
            "A. Increasing AI performance sometimes leads to greater model complexity, making it more difficult to explain decision-making processes.",
            "B. Increasing AI performance sometimes leads to certain evaluation metrics no longer being useful, making it more difficult to explain decision-making processes.",
            "C. Increasing AI performance sometimes removes human-in-the-loop (HITL) methods, making it more difficult to explain decision-making processes.",
            "D. Increasing AI performance sometimes reduces the transparency of input data used in training, making it more difficult to explain decision-making processes."
        ],
        answer: [0],
        explanation: ""
    },
    {
        id: "ite303c_164",
        question: "Which of the following explains why efficiency can sometimes incur systemic fragility?",
        options: [
            "A. Increased efficiency tends to create cost savings",
            "B.High-efficiency machines often require more maintenance",
            "C. Increased efficiency tends to compound over time",
            "D. Efficiency benefits may lead to complex second-order costs"
        ],
        answer: [3],
        explanation: ""
    },
    {
        id: "ite303c_165",
        question: "Which of the following are possible benefits of a human-in-the-loop (HITL) architecture? (Select two)",
        options: [
            "A. Eliminating the potential for human error in decision making",
            "B. Improving the speed of autonomous decision making",
            "C. Mitigating excessive scope or potential collateral damage",
            "D. Balancing the negative effects of an AI system on people with the effects on environments and objects"
        ],
        answer: [2, 3],
        explanation: ""
    },
    {
        id: "ite303c_166",
        question: "How does the \"virtuous cycle\" that benefits Big Tech operate?",
        options: [
            "A. Better classes of customers lead to richer and more refined data for algorithmic systems.",
            "B. By acting virtuous, the public respects Big Tech more and more.",
            "C. Data-driven algorithms improve solutions, leading to new customers, and better data.",
            "D. Organizations write algorithms with fewer biases, which leads to fairer outcomes."
        ],
        answer: [2],
        explanation: ""
    },
    {
        id: "ite303c_167",
        question: "Which of the following is often in opposition to moral relativism?",
        options: [
            "A. Customs and conventions",
            "B. Cultural mores",
            "C. Subjective perspectives",
            "D. Evidence-based policy"
        ],
        answer: [3],
        explanation: ""
    },
    {
        id: "ite303c_168",
        question: "Which of the following are important aspects of resolving complex and confounding business pressures? (Select two.)",
        options: [
            "A. Prioritizing ethical and safety concerns over business desires in all cases and situations",
            "B. Assuring everyone that their desires can be accommodated without compromise",
            "C. Managing expectations that not everyone can get what they want, when they want it",
            "D. Engaging with multiple stakeholders to understand their particular needs"
        ],
        answer: [2, 3],
        explanation: ""
    },
    {
        id: "ite303c_169",
        question: "You plan on streamlining your company's product experience, but you also want to uphold the agency and autonomy of your users. Which of the following actions would uphold these principles? (Select two)",
        options: [
            "A. Enabling government agencies to have a personalized interface with software",
            "B. Respecting the right of the user to choose and customize their experiences",
            "C. Applying machine intelligence to simulate customer behavior",
            "D. Refraining from guiding users into something they didn't wish for or intend"
        ],
        answer: [1, 3],
        explanation: ""
    },
    {
        id: "ite303c_170",
        question: "Which of the following describe corporate hegemony? (Select two)",
        options: [
            "A. Making multiple investments in a similar space to improve the outcomes of success",
            "B. Spending large sums on corporate branding and marketing",
            "C. Locking out smaller players, leading to monopolies or cartels",
            "D. Consolidating interests through mergers and acquisitions"
        ],
        answer: [2, 3],
        explanation: ""
    },
    {
        id: "ite303c_171",
        question: "Which school of philosophical thought primarily advocates for the greatest good for the greatest amount of people?",
        options: [
            "A. Utilitarianism",
            "B. Deontology",
            "C. Kantian ethics",
            "D. Virtue ethics"
        ],
        answer: [0],
        explanation: ""
    },
    {
        id: "ite303c_172",
        question: "Which type of entity are the OECD Principles on Artificial Intelligence mostly geared towards?",
        options: [
            "A. Individuals",
            "B. Private corporations",
            "C. National governments",
            "D. Municipal governments"
        ],
        answer: [2],
        explanation: ""
    },
    {
        id: "ite303c_173",
        question: "Which of the following principles are most commonly cited in AI-based ethical frameworks?",
        options: [
            "A. Happiness and spiritual contentment",
            "B. Human control and autonomy",
            "C. Fairness and non-discrimination",
            "D. Transparency and explainability"
        ],
        answer: [3],
        explanation: ""
    },
    {
        id: "ite303c_174",
        question: "Which of the following frameworks primarily promotes human rights?",
        options: [
            "A. The Montreal Declaration",
            "B. The Toronto Declaration",
            "C. The Asilomar AI Principles",
            "D. The Beijing AI Principles"
        ],
        answer: [1],
        explanation: ""
    },
    {
        id: "ite303c_175",
        question: "Which of the following best describes capability caution as referenced in the Asilomar AI Principles?",
        options: [
            "A. If there is no understanding of the internal mechanisms of AI, then AI development should be halted.",
            "B. Should there be a greater reliance on AI, measures should be taken to ensure that humans are still capable of finding work.",
            "C. We should keep limits on on what artificial general intelligence (AGI) is capable of.",
            "D. Given a lack of consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities."
        ],
        answer: [3],
        explanation: ""
    },
    {
        id: "ite303c_176",
        question: "Which of the following describes the principle of transparency in the context of AI systems?",
        options: [
            "A. Transparency enables human observers to understand the decision-making process of an AI system.",
            "B. Transparency enables human observers to tweak the decision-making process of an AI system.",
            "C. Transparency enables human observers to see inside the decision-making process of an AI system.",
            "D. Transparency enables human observers to reproduce the decision-making process of an AI system."
        ],
        answer: [2],
        explanation: ""
    },
    {
        id: "ite303c_177",
        question: "Which of the following is a case study that best represents the principle of professional responsibility?",
        options: [
            "A. The IEEE Ethically Aligned Design's discussion on classical ethics",
            "B. The Beijing AI Principles' tenets about the use of AI",
            "C. The Asilomar AI Principles' definition of capability caution",
            "D. The American Medical Association's definition of AI as augmented intelligence"
        ],
        answer: [3],
        explanation: ""
    },
    {
        id: "ite303c_178",
        question: "If you are attempting to build a new framework for the research and development (R&D) of AI, which of the following frameworks might you look at first for its emphasis in this area?",
        options: [
            "A. The G20 AI Principles",
            "B. The Beijing AI Principles",
            "C. The American Medical Association's definition of artificial intelligence",
            "D. The Montreal Declaration for a Responsible Development of Artificial Intelligence"
        ],
        answer: [1],
        explanation: ""
    },
    {
        id: "ite303c_179",
        question: "In AI, the principle of privacy is most commonly referred to in the context of which of the following concepts?",
        options: [
            "A. Data protection",
            "B. Transparency",
            "C. Personal protection",
            "D. Human control"
        ],
        answer: [0],
        explanation: ""
    },
    {
        id: "ite303c_180",
        question: "Which of the following describes an ethical framework?",
        options: [
            "A. Ethical frameworks raise timeless ethical questions that are not easily put into action.",
            "B. Ethical frameworks apply meta-ethical theories to everyday business operations.",
            "C. Ethical frameworks seek to mitigate ethical concerns by creating actionable steps.",
            "D. Ethical frameworks consolidate regulatory requirements for an industry."
        ],
        answer: [2],
        explanation: ""
    },
    {
        id: "ite303c_181",
        question: "Why does the trolley problem pose an ethical predicament?",
        options: [
            "A. You have to make a choice between two scenarios where choosing one leads to loss of life in the other.",
            "B. There are so many potential outcomes that it becomes difficult to choose one that is most ethical.",
            "C. The moral responsibility is split between you and the person controlling the trolley.",
            "D. You as the actor don't have sufficient control over the circumstance."
        ],
        answer: [0],
        explanation: ""
    },
    {
        id: "ite303c_182",
        question: "Which of the following ethical considerations should have priority in an emergency situation like the use of contact-tracing solutions during a pandemic? (Select two.)",
        options: [
            "A. Accountability",
            "B. Bias",
            "C. Privacy",
            "D. Explainability"
        ],
        answer: [0, 2],
        explanation: ""
    },
    {
        id: "ite303c_183",
        question: "If an AI-enabled system enables addictive behavior, which of the following makes for the most compelling argument to stop development work on that system?",
        options: [
            "A. The process for obtaining consent has not been made transparent to the user.",
            "B. The system, as designed, acts counter to the well-being of the users.",
            "C. There is a lack of accountability on the part of the user since they overuse the service.",
            "D. The user will share more data with the system because of increased use."
        ],
        answer: [1],
        explanation: ""
    },
    {
        id: "ite303c_184",
        question: "Why do smart toys raise additional ethical concerns over those that are raised in the course of other products and services that use AI?",
        options: [
            "A. They are used in the privacy of homes rather than in public settings, like other products or services.",
            "B. The smart toys store personal data on the device, which can be stolen.",
            "C. Children are more susceptible to manipulation and therefore need extra protective measures.",
            "D. It is difficult to obtain informed consent for the use of the smart toy."
        ],
        answer: [2],
        explanation: ""
    },
    {
        id: "ite303c_185",
        question: "Which of the following software development principles is essential in the real-world deployment of AI-enabled software applications in critical scenarios like self-driving cars?",
        options: [
            "A. Version control of the AI models deployed",
            "B. Continuous integration and deployment of patch updates",
            "C. Robustness to adversarial examples",
            "D. Architectural design analysis"
        ],
        answer: [2],
        explanation: ""
    },
    {
        id: "ite303c_186",
        question: "Which of the following is the most important argument in favor of content moderation in online platforms?",
        options: [
            "A. It prevents the development of monopolies in terms of content creators.",
            "B. It prevents the spread of disinformation that can cause harm to vulnerable populations.",
            "C. It creates adequate incentives for everyone to share their opinions.",
            "D. It helps uphold freedom of expression for everyone and doesn't give anyone special rights."
        ],
        answer: [1],
        explanation: ""
    },
    {
        id: "ite303c_187",
        question: "Why is the question of robot rights and emancipation one that isn't as important as addressing issues of bias, privacy, transparency, and other principles discussed in the various ethical frameworks?",
        options: [
            "A. These rights necessitate that robots become sentient entities, which is currently not feasible.",
            "B. There is no legal precedent for granting rights to entities that are not humans.",
            "C. Robots are mechanical instruments and therefore don't deserve to have rights.",
            "D. Humans are anthropocentric and don't want to extend rights to other sentient entities."
        ],
        answer: [0],
        explanation: ""
    },
    {
        id: "ite303c_188",
        question: "Which of the following is the most important ethical consideration regarding technical developments like deepfakes?",
        options: [
            "A. They violate data sharing agreements in many jurisdictions.",
            "B. They are built on technological progress made by a third-party organization.",
            "C. They usurp a person's likeness and can then be weaponized against them.",
            "D. They take away monetization opportunities, leaving individuals unfairly compensated for their data."
        ],
        answer: [2],
        explanation: ""
    },
    {
        id: "ite303c_189",
        question: "Why are anonymization and pseudonymization insufficient protection measures against breaches of data privacy and security?",
        options: [
            "A. They destroy the usefulness of the data.",
            "B. They only work in scenarios with particular kinds of personal information.",
            "C. They can be broken by combining this data with other publicly available data.",
            "D. They don't integrate well into data science and machine learning workflows."
        ],
        answer: [2],
        explanation: ""
    },
    {
        id: "ite303c_190",
        question: "In using AI-enabled solutions within the context of medical imaging analysis, which of the following is the most important ethical consideration?",
        options: [
            "A. Privacy",
            "B. Explainability",
            "C. Bias",
            "D. Security"
        ],
        answer: [1],
        explanation: ""
    },
    {
        id: "ite303c_191",
        question: "Which of the following are ways that regulations differ from ethical frameworks? (Select two.)",
        options: [
            "A. Regulations are flexible in their implementation.",
            "B. Regulations provide a clear basis for potential litigation.",
            "C. Regulations have legal enforcement behind them.",
            "D. Regulations are often industry led."
        ],
        answer: [1, 2],
        explanation: ""
    },
    {
        id: "ite303c_192",
        question: "Your business handles the personal data of California residents. Which of the following regulations would enable a resident to request that their data be deleted from your company's files?",
        options: [
            "A. CCPA",
            "B. OECD Privacy Guidelines",
            "C. PCI DSS",
            "D. COPPA"
        ],
        answer: [0],
        explanation: ""
    },
    {
        id: "ite303c_193",
        question: "Which of the following does the Brazilian General Data Protection Act (LGPD) mandate? (Select two.)",
        options: [
            "A. Data protection analysts",
            "B. Data protection officers",
            "C. Data protection audits",
            "D. Data protection impact assessments"
        ],
        answer: [1, 3],
        explanation: ""
    },
    {
        id: "ite303c_194",
        question: "How does increasing AI performance often conflict with the desire for explainability?",
        options: [
            "A. Increasing AI performance sometimes reduces the transparency of input data used in training, making it more difficult to explain decision-making processes.",
            "B. Increasing AI performance sometimes leads to greater model complexity, making it more difficult to explain decision-making processes.",
            "C. Increasing AI performance sometimes leads to certain evaluation metrics no longer being useful, making it more difficult to explain decision-making processes.",
            "D. Increasing AI performance sometimes removes human-in-the-loop (HITL) methods, making it more difficult to explain decision-making processes."
        ],
        answer: [1],
        explanation: ""
    },
    {
        id: "ite303c_195",
        question: "Which of the following explains why efficiency can sometimes incur systemic fragility?",
        options: [
            "A. Increased efficiency tends to compound over time",
            "B. Increased efficiency tends to create cost savings",
            "C. Efficiency benefits may lead to complex second-order costs",
            "D. High-efficiency machines often require more maintenance"
        ],
        answer: [2],
        explanation: ""
    },
    {
        id: "ite303c_196",
        question: "Which of the following are possible benefits of a human-in-the-loop (HITL) architecture? (Select two.)",
        options: [
            "A. Improving the speed of autonomous decision making",
            "B. Eliminating the potential for human error in decision making",
            "C. Mitigating excessive scope or potential collateral damage",
            "D. Balancing the negative effects of an AI system on people with the effects on environments and objects"
        ],
        answer: [2, 3],
        explanation: ""
    },
    {
        id: "ite303c_197",
        question: "Which of the following is not a valid risk response technique?",
        options: [
            "a. Accept",
            "b. Ignore",
            "c. Transfer",
            "d. Avoid"
        ],
        answer: [1],
        explanation: ""
    },
    {
        id: "ite303c_198",
        question: "How do AI and other data-driven technologies use probability?",
        options: [
            "A. By determining the objective likelihood of some event happening",
            "B. By providing a model of belief about the likelihood of some event happening",
            "C. By guaranteeing that some event will occur with 100% likelihood",
            "D. By estimating the likelihood of some event happening without input data"
        ],
        answer: [1],
        explanation: ""
    },
    {
        id: "ite303c_199",
        question: "You have a dataset of customers that includes each customer's gender, location, and other personal attributes. The label you are trying to predict is how much sales revenue each customer is likely to generate for the business based on these attributes. What type of machine learning outcome is this problem suited for?",
        options: [
            "A. Regression",
            "B. Clustering",
            "C. Classification",
            "D. Dimensionality reduction"
        ],
        answer: [0],
        explanation: ""
    },
    {
        id: "ite303c_200",
        question: "You're training a model to classify whether or not a bridge is likely to collapse given several factors. You have a dataset of thousands of existing bridges and their attributes, where each bridge is labeled as having collapsed or not collapsed. Only a handful of bridges in the dataset are labeled as having collapsedthe rest are labeled as not collapsed. Given your goal of minimizing bridge collapse and the severe harm it can cause, which of the following metrics would be most useful for evaluating the model?",
        options: [
            "A. Confusion matrix",
            "B. Accuracy",
            "C. Precision",
            "D. Recall"
        ],
        answer: [3],
        explanation: ""
    },
    {
        id: "ite303c_201",
        question: "Which of the following metrics is used to evaluate a linear regression machine learning model?",
        options: [
            "A. Goodhart's Law",
            "B. Receiver operating characteristic (ROC)",
            "C. Cost function",
            "D. Accuracy"
        ],
        answer: [2],
        explanation: ""
    }
];
